<!DOCTYPE html><html>

<head>
<meta charset="utf-8">
<title>P4_report</title>
<style type="text/css">
*{margin:0;padding:0;}
body {
	font:13.34px helvetica,arial,freesans,clean,sans-serif;
	color:black;
	line-height:1.4em;
	background-color: #F8F8F8;
	padding: 0.7em;
}
p {
	margin:1em 0;
	line-height:1.5em;
}
table {
	font-size:inherit;
	font:100%;
	margin:1em;
}
table th{border-bottom:1px solid #bbb;padding:.2em 1em;}
table td{border-bottom:1px solid #ddd;padding:.2em 1em;}
input[type=text],input[type=password],input[type=image],textarea{font:99% helvetica,arial,freesans,sans-serif;}
select,option{padding:0 .25em;}
optgroup{margin-top:.5em;}
pre,code{font:12px Monaco,"Courier New","DejaVu Sans Mono","Bitstream Vera Sans Mono",monospace;}
pre {
	margin:1em 0;
	font-size:12px;
	background-color:#eee;
	border:1px solid #ddd;
	padding:5px;
	line-height:1.5em;
	color:#444;
	overflow:auto;
	-webkit-box-shadow:rgba(0,0,0,0.07) 0 1px 2px inset;
	-webkit-border-radius:3px;
	-moz-border-radius:3px;border-radius:3px;
}
pre code {
	padding:0;
	font-size:12px;
	background-color:#eee;
	border:none;
}
code {
	font-size:12px;
	background-color:#f8f8ff;
	color:#444;
	padding:0 .2em;
	border:1px solid #dedede;
}
img{border:0;max-width:100%;}
abbr{border-bottom:none;}
a{color:#4183c4;text-decoration:none;}
a:hover{text-decoration:underline;}
a code,a:link code,a:visited code{color:#4183c4;}
h2,h3{margin:1em 0;}
h1,h2,h3,h4,h5,h6{border:0;}
h1{font-size:170%;border-top:4px solid #aaa;padding-top:.5em;margin-top:1.5em;}
h1:first-child{margin-top:0;padding-top:.25em;border-top:none;}
h2{font-size:150%;margin-top:1.5em;border-top:4px solid #e0e0e0;padding-top:.5em;}
h3{margin-top:1em;}
hr{border:1px solid #ddd;}
ul{margin:1em 0 1em 2em;}
ol{margin:1em 0 1em 2em;}
ul li,ol li{margin-top:.5em;margin-bottom:.5em;}
ul ul,ul ol,ol ol,ol ul{margin-top:0;margin-bottom:0;}
blockquote{margin:1em 0;border-left:5px solid #ddd;padding-left:.6em;color:#555;}
dt{font-weight:bold;margin-left:1em;}
dd{margin-left:2em;margin-bottom:1em;}
sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}
* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
}
</style>
</head>
<body>
<h3 id="toc_0">Project 4: Identifying Fraud from Enron Email  (Intro Machine Learning)</h3>

<ul>
<li>Author:  Chi-Yuan Cheng (cyuancheng AT gmail DOT com) </li>
<li>Last updated: August 4, 2015</li>
</ul>

<h3 id="toc_1">1. Information</h3>

<p>Enron was an infamous American company which was known for its extensive fraud. Enron marketed electricity and natural gas, delivered energy and other physical commodities around the world. It was once ranked the sixth largest energy company in the world. Most of the top Enron&#39;s executives were tried for fraud after it was revealed in November 2001 that Enron&#39;s earnings had been overstated by several hundred million dollars. Enron paid the top 140 executives $680 million in 2001. Among others, Kenneth Lay (founder, chairman of the Enron board directors) received $67.4 million and Jeffrey Skilling (former CEO) received $41.8 million. Moreover, the top executives sold their company stock prior to the company&#39;s downfall. The Enron scandal was one of the five largest audit and accountancy partnerships in the world and Enron was cited as the biggest audit failure.</p>

<p>On the other hands, the Enron Corpus is believed to be one of the largest publicly available collection of real-world email data. It has been widely used for research in social network analysis, natural language processing, and machine learning. The Enron financial records of former executives and employees were also released during the fraud trials.  </p>

<p>The goal of this project is to develop machine learning algorithm, together with scikit-learn Python module, to predict the person of interest (POI) of a fraud from the email and financial (E+F) dataset. POIs were ‘individuals who were indicted, reached a settlement, or plea deal with the government, or testified in exchange for prosecution immunity.’  The E+F dataset from the Enron Corpus was used as features for the POI prediction.</p>

<h3 id="toc_2">2. The Enron Data</h3>

<p>Enron E + F dataset, which can be download on the Internet.</p>

<p>It contains:</p>

<ul>
<li> Length of dataset: 146</li>
<li> Number of feature: 21</li>
<li> POIs : 18 (the poi feature set as <code>True</code>)</li>
</ul>

<p>There are many features with missing values (NaN) in the dataset:</p>

<table>
<thead>
<tr>
<th>features</th>
<th style="text-align: center">Numbers of NaN</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>loan_advances</code></td>
<td style="text-align: center">142</td>
</tr>
<tr>
<td><code>director_fees</code></td>
<td style="text-align: center">129</td>
</tr>
<tr>
<td><code>restricted_stock_deferred</code></td>
<td style="text-align: center">128</td>
</tr>
<tr>
<td><code>deferral_payments</code></td>
<td style="text-align: center">107</td>
</tr>
<tr>
<td><code>deferred_income</code></td>
<td style="text-align: center">97</td>
</tr>
<tr>
<td><code>long_term_incentive</code></td>
<td style="text-align: center">80</td>
</tr>
<tr>
<td><code>bonus</code></td>
<td style="text-align: center">64</td>
</tr>
<tr>
<td><code>shared_receipt_with_poi</code></td>
<td style="text-align: center">60</td>
</tr>
<tr>
<td><code>from_this_person_to_poi</code></td>
<td style="text-align: center">60</td>
</tr>
<tr>
<td><code>from_messages</code></td>
<td style="text-align: center">60</td>
</tr>
<tr>
<td><code>to_messages</code></td>
<td style="text-align: center">60</td>
</tr>
<tr>
<td><code>from_poi_to_this_person</code></td>
<td style="text-align: center">60</td>
</tr>
<tr>
<td><code>other</code></td>
<td style="text-align: center">53</td>
</tr>
<tr>
<td><code>expenses</code></td>
<td style="text-align: center">51</td>
</tr>
<tr>
<td><code>salary</code></td>
<td style="text-align: center">51</td>
</tr>
<tr>
<td><code>exercised_stock_options</code></td>
<td style="text-align: center">44</td>
</tr>
<tr>
<td><code>restricted_stock</code></td>
<td style="text-align: center">36</td>
</tr>
<tr>
<td><code>email_address</code></td>
<td style="text-align: center">35</td>
</tr>
<tr>
<td><code>total_payments</code></td>
<td style="text-align: center">21</td>
</tr>
<tr>
<td><code>total_stock_value</code></td>
<td style="text-align: center">20</td>
</tr>
</tbody>
</table>

<p>The features in the dataset are listed below:</p>

<p><em>financial feature:</em></p>

<pre><code>&#39;bonus&#39;, &#39;deferral_payments&#39;, &#39;deferred_income&#39;, &#39;director_fees&#39;, 
&#39;exercised_stock_options&#39;, &#39;expenses&#39;, &#39;loan_advances&#39;, &#39;long_term_incentive&#39;,
 &#39;other&#39;, &#39;restricted_stock&#39;, &#39;restricted_stock_deferred&#39;,
 &#39;salary&#39;, &#39;total_payments&#39;, &#39;total_stock_value&#39;</code></pre>

<p><em>email feature:</em></p>

<pre><code>&#39;email_address&#39;, &#39;from_messages&#39;, &#39;from_poi_to_this_person&#39;, 
&#39;from_this_person_to_poi&#39;, &#39;to_messages&#39;, &#39;shared_receipt_with_poi&#39;, </code></pre>

<h3 id="toc_3">3. Feature Processing</h3>

<h4 id="toc_4">Outliers</h4>

<p>I removed the following entries in the dataset.</p>

<ol>
<li>The <code>TOTAL</code>  is the summation of all the data points for each feature, and it is the biggest Enron E+F dataset outlier. </li>
<li>The<code>THE TRAVEL AGENCY IN THE PARK</code> is a travel agency, not the name of employee at Enron.</li>
<li>The <code>LOCKHART EUGENE E</code> missed all the feature values, and is not very useful in the dataset.</li>
<li>The <code>email_address</code> feature is also a outliner, because it is a person&#39;s email address and is not useful to identify POIs.</li>
</ol>

<p>This dataset contains lots of missing values (NaN). Many machine learning models don&#39;t like NaN. If we just fill the NaN values with zero, it may bias the data towards low values. The solution here is to fill the NaN values with median values for each feature.</p>

<p>In addition, there are three outliers in the features of salary and bonus. Two people (Jeffrey Skilling, Kenneth Lay) made salaries more than 1 million dollars, and two people (John Lavorato, Kenneth Lay) made a bonus of over 7 million dollars. However, I decided to not remove these extreme values from the dataset. It is because these extreme values might significantly affect the fraud at Enron and could be key parameters to identity the POIs.</p>

<h4 id="toc_5">New features</h4>

<p>We expect that POIs contact with each other more frequently than with non-POIs, so this might be key information to predict POIs. Therefore, I engineered the following three features:</p>

<table>
<thead>
<tr>
<th>new  features</th>
<th style="text-align: center">description</th>
</tr>
</thead>

<tbody>
<tr>
<td><code>fraction from poi</code></td>
<td style="text-align: center">fraction of messages from a PO to that person I</td>
</tr>
<tr>
<td><code>fraction to poi</code></td>
<td style="text-align: center">fraction of messages from that person to a POI</td>
</tr>
<tr>
<td><code>fraction email with poi</code></td>
<td style="text-align: center">fraction of messages to and from that person to a POI</td>
</tr>
</tbody>
</table>

<p>As will be evaluated in more detail later, these new features did not improve the overall performance of the final algorithm, so they were not included in the final model.</p>

<h4 id="toc_6">Feature Selection</h4>

<p>Before feeding the features into any models, it is necessary to scale the feature values to be between 0 and 1. It is because if one feature has a broad range of values, the outcome will be governed by this particular feature. Therefore, the range of all feature values need to be normalized, so that each feature contributes approximately proportionately to the final outcome.</p>

<p>I used SelectKBest to search the best features in E+F feature list, according to the ANOVA F-value classification scoring function, as below. This step can get rid of some unnecessary features that may overfit the predictive model.</p>

<p><img src="Fig_SelectKBest_features.png" alt="selected features" title="Selected features"></p>

<p>The top 15 features from SelectKBest were fed to the DecisionTreeClassifier algorithm to determine the feature importance. </p>

<p><img src="Fig_importance.png" alt="feature importance" title="feature importance"></p>

<p>I then removed the features by hand to optimize the score of the model. 
Finally, I chose the following features:</p>

<pre><code>&#39;exercised_stock_options&#39;, &#39;total_stock_value&#39;,
 &#39;bonus&#39;, &#39;salary&#39;,  &#39;deferred_income&#39;</code></pre>

<h3 id="toc_7">4. Algorithm Selection and Tuning</h3>

<h4 id="toc_8">Preliminary test</h4>

<p>Initially, I fed the top 5 features that were found by SelectKBest to the following six algorithms with default parameters and compared their scores.</p>

<table>
<thead>
<tr>
<th>Algorithm</th>
<th style="text-align: center">Accuracy</th>
<th style="text-align: center">Precision</th>
<th style="text-align: center">Recall</th>
</tr>
</thead>

<tbody>
<tr>
<td>Logistic Regression</td>
<td style="text-align: center">0.8747</td>
<td style="text-align: center">0.2100</td>
<td style="text-align: center">0.2100</td>
</tr>
<tr>
<td>LinearSVC</td>
<td style="text-align: center">0.8727</td>
<td style="text-align: center">0.3433</td>
<td style="text-align: center">0.3433</td>
</tr>
<tr>
<td>KNeighbors</td>
<td style="text-align: center">0.8680</td>
<td style="text-align: center">0.1550</td>
<td style="text-align: center">0.1550</td>
</tr>
<tr>
<td>Random Forest</td>
<td style="text-align: center">0.8547</td>
<td style="text-align: center">0.2720</td>
<td style="text-align: center">0.2961</td>
</tr>
<tr>
<td>AdaBoost</td>
<td style="text-align: center">0.8440</td>
<td style="text-align: center">0.3598</td>
<td style="text-align: center">0.3448</td>
</tr>
<tr>
<td>Decision Tree</td>
<td style="text-align: center">0.7800</td>
<td style="text-align: center">0.1955</td>
<td style="text-align: center">0.2120</td>
</tr>
</tbody>
</table>

<p><img src="Figure_models_test_initial.png" alt="model test" title="model test"></p>

<p>These models give high accuracies (&gt;0.78), but they have low precision and recall (0.1~0.3). Among these tested models, <em>LinearSVC</em> and <em>AdaBoost</em> have precision and recall scores above 0.3.  </p>

<h4 id="toc_9">Analysis Validation and Performance</h4>

<p>Tuning the parameters of algorithm means to optimize the performance of the model to the particular features in the dataset. The performance can be determined by accuracy, precision, and recall scores. Here, I fined tune the parameters of <strong><em>LinearSVC</em></strong> and  <strong><em>AdaBoost</em></strong>  to get the optimal performance of the models.  </p>

<p>To optimize the performance of model, I first scales the selected features to be between 0 and 1 using MinMaxScaler. The scaled features was fed into Principal Components Analysis (PCA) dimensional reduction algorithm as a part of GridSearchCV pipeline, when searching the optimal estimator parameters for particular classification algorithm. These two steps were used during each cross-validation step for the grid search and optimal parameters. </p>

<p>Finally, I end up using <strong><em>AdaBoost</em></strong> (with optimal Decision Tree estimator) , because its scores of precision and recall are the highest among other algorithms (&gt; 0.3). The scores using for optimal AdaBoost and LinearSVC are below</p>

<table>
<thead>
<tr>
<th>Algorithm</th>
<th style="text-align: center">Accuracy</th>
<th style="text-align: center">Precision</th>
<th style="text-align: center">Recall</th>
</tr>
</thead>

<tbody>
<tr>
<td>AdaBoost</td>
<td style="text-align: center">0.8227</td>
<td style="text-align: center">0.3318</td>
<td style="text-align: center">0.3650</td>
</tr>
<tr>
<td>LinearSVC</td>
<td style="text-align: center">0.8733</td>
<td style="text-align: center">0.3433</td>
<td style="text-align: center">0.2200</td>
</tr>
</tbody>
</table>

<p>The optimized parameters for AdaBoost is:</p>

<pre><code>Pipeline(steps=[(&#39;pca&#39;, PCA(copy=True, n_components=3, whiten=False)), (&#39;clf&#39;, AdaBoostClassifier(algorithm=&#39;SAMME.R&#39;,
          base_estimator=DecisionTreeClassifier(class_weight=None, criterion=&#39;gini&#39;, max_depth=None,
            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,
            min_samples_split=0.316227766017, min_weight_fraction_leaf=0.0,
            random_state=42, splitter=&#39;random&#39;),
          learning_rate=0.5, n_estimators=1, random_state=42))])
</code></pre>

<p>The classic way of doing <em>validation</em> is to separate dataset into a training set and a testing set, and train the specific model on the training set. Then we test our model on the testing set to see how the performance evaluated from predicted values of the testing set match up the actual values for the testing set. Although this strategy can avoid overfitting data to our models, validation can go wrong if data splitting is biased. For example, in some cases, valuable information is only contained in the test set, not in the training set, or the size of the dataset is very limited. A proper way to solve these problems are to randomly select the observations for the test set (Stratified Shuffle Split validation) or to split dataset into multiple-consecutive sets (k-fold validation), and take the average of the testing results generated by the classifier. </p>

<p>In this project, I validated the dataset using StratifiedShuffleSplit function provided by sklearn. The n_iter (represented by the folds variable) was set to 100, and  I used 90% data to train the model and 10% data to test the model. That means the model randomly samples 90% (10%) data to train (test) the model for each iteration, and averaged the scores for the model validation. </p>

<p>Finally, let&#39;s compare the performance of final model before and after adding the new features. </p>

<p><img src="Figure_new_features.png" alt="new features" title="new features"></p>

<p>I found that these new features do not significantly improve the performance of the final model; therefore, they are not include in the feature list.</p>

<h3 id="toc_10">5. Discussion and Conclusions</h3>

<ul>
<li><p>Accuracy: It represents the ratio of correct prediction out of the total prediction made. In other words, it means how many percentage of POIs the model can correctly predict. The average accuracy for optimal AdaBoost model is 0.8227, which means 82.27 % of predictions this model made were correct.</p></li>
<li><p>Precision: It represents the ratio of correct positive prediction (True positives) made out of the total positive prediction made (True positives + False positives) . Here, the positive prediction means predicting the employee who is a POI (POI returns &quot;1&quot;). The average precision of the optimal AdaBoost model is 0.3318, which means that 33.18 % of the total positive prediction made by model were correct.</p></li>
<li><p>Recall: It represents the ratio of correct positive prediction (True positives) made out of the actual total predictions, that were indeed positive (True positives + False negatives). The recall value in this project means what percentage of actual POIs in the test dataset were correctly identified. The optimal AdaBoost model achieves a recall of 0.3650, which means only 36.50 % of POI were correctly identified among all the actual POIs.</p></li>
</ul>

<p>In many cases, the accuracy can be pretty good (&gt;80%), but still lacking good precision or recall (&lt;30%). It is important to note that a model has a very high accuracy does not mean it is a good model. There is also a trade-off between precision and recall, which needs to be balanced in the models. However, the Enron E+F dataset contains much more POIs than non-POIs (18 vs 127). With this unbalanced dataset, the precision and recall are useful metrics for evaluating the model. For example, the original Enron data contains 18 POIs over 145 employee. The optimal AdaBoost model can successfully identify 15 individual as POIs, where only 5 of them actually to be POIs. More data will definitely help to improve the performance and validation of models. </p>

<h3 id="toc_11">6. References</h3>

<ol>
<li><a href="https://en.wikipedia.org/wiki/Enron_scandal">Enron scandal, from WiKipedia</a></li>
<li><a href="https://www.cs.cmu.edu/%7E./enron/">Enron Email Dataset</a></li>
<li><a href="http://news.findlaw.com/legalnews/lit/enron/">Enron Financial Dataset</a></li>
<li><a href="http://enrondata.org/content/research/">EnronData.org</a></li>
<li><a href="http://scikit-learn.org/stable/">scikit-learn</a></li>
<li>Python for Data Analysis, by Wes McKinney, O&#39;Reilly Media, 2012.</li>
</ol>


</body>

</html>
